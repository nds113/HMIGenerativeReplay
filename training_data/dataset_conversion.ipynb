{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "split= \"train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ouranos/miniconda3/envs/hmi/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for sst contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/sst\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the SST dataset from Hugging Face datasets\n",
    "sst_dataset = load_dataset(\"sst\")\n",
    "\n",
    "# Extract the necessary information from the SST dataset\n",
    "reformatted_data = []\n",
    "for example in sst_dataset[split]:\n",
    "    sentence = example[\"sentence\"]\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    # Create a new JSON object following the SQuAD format\n",
    "    squad_example = {\n",
    "        \"context\": sentence,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": \"What is the sentiment of this sentence?\",\n",
    "                \"id\": f\"sst-{len(reformatted_data)}\",\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": \"Positive\" if label == 1 else \"Negative\",\n",
    "                        \"answer_start\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reformatted_data.append(squad_example)\n",
    "\n",
    "# Create the final JSON object\n",
    "squad_formatted_data = {\n",
    "    \"version\": \"1.1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"SST Dataset\",\n",
    "            \"paragraphs\": reformatted_data\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the reformatted data to a JSON file\n",
    "with open(f\"/home/ouranos/Documents/Classes/NLP/HMIGenerativeReplay/training_data/sst_to_squad-{split}-v2.0.json\", \"w\") as file:\n",
    "    json.dump(squad_formatted_data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ouranos/miniconda3/envs/hmi/lib/python3.10/site-packages/datasets/load.py:1486: FutureWarning: The repository for qa_srl contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/qa_srl\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the QA-SRL dataset from Hugging Face datasets\n",
    "qasrl_dataset = load_dataset(\"qa_srl\")\n",
    "\n",
    "# Extract the necessary information from the QA-SRL dataset\n",
    "reformatted_data = []\n",
    "for example in qasrl_dataset[split]:\n",
    "    sentence = example[\"sentence\"]\n",
    "    sent_id = example[\"sent_id\"]\n",
    "    predicate = example[\"predicate\"]\n",
    "    question_template = example[\"question\"]\n",
    "    answers = example[\"answers\"]\n",
    "    \n",
    "    # Format the question\n",
    "    question = \" \".join([token for token in question_template if token != \"_\"]).capitalize()\n",
    "    \n",
    "    # Create answer objects\n",
    "    squad_answers = []\n",
    "    for answer in answers:\n",
    "        answer_start = sentence.find(answer)\n",
    "        squad_answers.append({\"text\": answer, \"answer_start\": answer_start})\n",
    "    \n",
    "    # Create a new JSON object following the SQuAD format\n",
    "    squad_example = {\n",
    "        \"context\": sentence,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"id\": f\"{sent_id}-{predicate}\",\n",
    "                \"answers\": squad_answers\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reformatted_data.append(squad_example)\n",
    "\n",
    "# Create the final JSON object\n",
    "squad_formatted_data = {\n",
    "    \"version\": \"1.1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"QA-SRL Dataset\",\n",
    "            \"paragraphs\": reformatted_data\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the reformatted data to a JSON file\n",
    "with open(f\"/home/ouranos/Documents/Classes/NLP/HMIGenerativeReplay/training_data/srl_to_squad-{split}-v2.0.json\", \"w\") as file:\n",
    "    json.dump(squad_formatted_data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the DBpedia dataset from Hugging Face datasets\n",
    "dbpedia_dataset = load_dataset(\"dbpedia_14\")\n",
    "\n",
    "# Extract the necessary information from the DBpedia dataset\n",
    "reformatted_data = []\n",
    "for example in dbpedia_dataset[split]:\n",
    "    text = example[\"content\"]\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    # Create a new JSON object following the SQuAD format\n",
    "    squad_example = {\n",
    "        \"context\": text,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": \"What is the category of this text?\",\n",
    "                \"id\": f\"dbpedia-{len(reformatted_data)}\",\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": str(label),\n",
    "                        \"answer_start\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reformatted_data.append(squad_example)\n",
    "\n",
    "# Create the final JSON object\n",
    "squad_formatted_data = {\n",
    "    \"version\": \"1.1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"DBpedia Dataset\",\n",
    "            \"paragraphs\": reformatted_data\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the reformatted data to a JSON file\n",
    "with open(f\"/home/ouranos/Documents/Classes/NLP/HMIGenerativeReplay/training_data/dbpedia_to_squad-{split}-v2.0.json\", \"w\") as file:\n",
    "    json.dump(squad_formatted_data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the AG News dataset from Hugging Face datasets\n",
    "ag_news_dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "# Extract the necessary information from the AG News dataset\n",
    "reformatted_data = []\n",
    "for example in ag_news_dataset[split]:\n",
    "    text = example[\"text\"]\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    # Map the label to the corresponding category\n",
    "    label_map = {0: \"World\", 1: \"Sports\", 2: \"Business\", 3: \"Sci/Tech\"}\n",
    "    category = label_map[label]\n",
    "    \n",
    "    # Create a new JSON object following the SQuAD format\n",
    "    squad_example = {\n",
    "        \"context\": text,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": \"What is the category of this news article?\",\n",
    "                \"id\": f\"ag_news-{len(reformatted_data)}\",\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": category,\n",
    "                        \"answer_start\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reformatted_data.append(squad_example)\n",
    "\n",
    "# Create the final JSON object\n",
    "squad_formatted_data = {\n",
    "    \"version\": \"1.1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"AG News Dataset\",\n",
    "            \"paragraphs\": reformatted_data\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the reformatted data to a JSON file\n",
    "with open(f\"/home/ouranos/Documents/Classes/NLP/HMIGenerativeReplay/training_data/ag_to_squad-{split}-v2.0.json\", \"w\") as file:\n",
    "    json.dump(squad_formatted_data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Yelp Review Full dataset from Hugging Face datasets\n",
    "yelp_dataset = load_dataset(\"codyburker/yelp_review_sampled\")\n",
    "\n",
    "# Extract the necessary information from the Yelp dataset\n",
    "reformatted_data = []\n",
    "for example in yelp_dataset[\"train\"]:\n",
    "    text = example[\"text\"]\n",
    "    label = example[\"stars\"]\n",
    "    \n",
    "    # Map the label to the corresponding star rating\n",
    "    label_map = {1: \"1 star\", 2: \"2 stars\", 3: \"3 stars\", 4: \"4 stars\", 5: \"5 stars\"}\n",
    "    star_rating = label_map[label]\n",
    "    \n",
    "    # Create a new JSON object following the SQuAD format\n",
    "    squad_example = {\n",
    "        \"context\": text,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": \"What is the star rating of this Yelp review?\",\n",
    "                \"id\": f\"yelp-{len(reformatted_data)}\",\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": star_rating,\n",
    "                        \"answer_start\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reformatted_data.append(squad_example)\n",
    "\n",
    "# Create the final JSON object\n",
    "squad_formatted_data = {\n",
    "    \"version\": \"1.1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"Yelp Review Full Dataset\",\n",
    "            \"paragraphs\": reformatted_data\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save the reformatted data to a JSON file\n",
    "with open(f\"/home/ouranos/Documents/Classes/NLP/HMIGenerativeReplay/training_data/yelp_to_squad-{split}-v2.0.json\", \"w\") as file:\n",
    "    json.dump(squad_formatted_data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the WikiSQL dataset from Hugging Face datasets\n",
    "wikisql_dataset = load_dataset(\"wikisql\")\n",
    "\n",
    "# Extract the necessary information from the WikiSQL dataset\n",
    "reformatted_data = []\n",
    "for example in wikisql_dataset[split]:\n",
    "    question = example[\"question\"]\n",
    "    table = example[\"table\"]\n",
    "    sql = example[\"sql\"]\n",
    "    \n",
    "    # Extract the table header and rows\n",
    "    header = table[\"header\"]\n",
    "    rows = table[\"rows\"]\n",
    "    \n",
    "    # Create a context string from the table information\n",
    "    context = f\"Table:\\n\"\n",
    "    context += \" | \".join(header) + \"\\n\"\n",
    "    for row in rows:\n",
    "        context += \" | \".join(str(cell) for cell in row) + \"\\n\"\n",
    "    \n",
    "    # Extract the SQL query components\n",
    "    sel_col = sql[\"sel\"]\n",
    "    agg_op = sql[\"agg\"]\n",
    "    cond_col_index = sql[\"conds\"][\"column_index\"]\n",
    "    cond_op_index = sql[\"conds\"][\"operator_index\"]\n",
    "    cond_value = sql[\"conds\"][\"condition\"]\n",
    "    \n",
    "    # Generate the SQL query string\n",
    "    sql_query = f\"SELECT {header[sel_col]} FROM table\"\n",
    "    if cond_col_index and cond_op_index and cond_value:\n",
    "        sql_query += f\" WHERE {header[cond_col_index[0]]} = {cond_value[0]}\"\n",
    "    \n",
    "    # Create a new JSON object following the SQuAD format\n",
    "    squad_example = {\n",
    "        \"context\": context,\n",
    "        \"qas\": [\n",
    "            {\n",
    "                \"question\": question,\n",
    "                \"id\": f\"wikisql-{len(reformatted_data)}\",\n",
    "                \"answers\": [\n",
    "                    {\n",
    "                        \"text\": sql_query,\n",
    "                        \"answer_start\": 0\n",
    "                    }\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    reformatted_data.append(squad_example)\n",
    "\n",
    "# Create the final JSON object\n",
    "squad_formatted_data = {\n",
    "    \"version\": \"1.1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"WikiSQL Dataset\",\n",
    "            \"paragraphs\": reformatted_data\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f\"/home/ouranos/Documents/Classes/NLP/HMIGenerativeReplay/training_data/wikisql_to_squad-{split}-v2.0.json\", \"w\") as file:\n",
    "    json.dump(squad_formatted_data, file, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ouranos/miniconda3/envs/hmi/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load the WoZ dataset from Hugging Face datasets\n",
    "woz_dataset = load_dataset(\"woz_dialogue\",'en')\n",
    "\n",
    "# Extract the necessary information from the WoZ dataset\n",
    "reformatted_data = []\n",
    "for example in woz_dataset[\"train\"]:\n",
    "    dialogue = example[\"dialogue\"]\n",
    "    \n",
    "    # Extract the user utterances and system responses\n",
    "    context = \"\"\n",
    "    qas = []\n",
    "    for turn in dialogue:\n",
    "        user_utterance = turn[\"transcript\"]\n",
    "        system_response = turn[\"system_transcript\"]\n",
    "        \n",
    "        # Append the user utterance and system response to the context\n",
    "        context += f\"User: {user_utterance}\\n\"\n",
    "        context += f\"System: {system_response}\\n\"\n",
    "        \n",
    "        # Create a question-answer pair for each turn\n",
    "        qa = {\n",
    "            \"question\": user_utterance,\n",
    "            \"id\": f\"woz-{example['dialogue_idx']}-{turn['turn_idx']}\",\n",
    "            \"answers\": [\n",
    "                {\n",
    "                    \"text\": system_response,\n",
    "                    \"answer_start\": context.find(system_response)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "        qas.append(qa)\n",
    "    \n",
    "    # Create a new JSON object following the SQuAD format\n",
    "    squad_example = {\n",
    "        \"context\": context,\n",
    "        \"qas\": qas\n",
    "    }\n",
    "    \n",
    "    reformatted_data.append(squad_example)\n",
    "\n",
    "# Create the final JSON object\n",
    "squad_formatted_data = {\n",
    "    \"version\": \"1.1\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"title\": \"WoZ Dataset\",\n",
    "            \"paragraphs\": reformatted_data\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "with open(f\"/home/ouranos/Documents/Classes/NLP/HMIGenerativeReplay/training_data/woz.en_to_squad-{split}-v2.0.json\", \"w\") as file:\n",
    "    json.dump(squad_formatted_data, file, indent=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hmi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
